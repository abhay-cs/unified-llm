# Architecture Breakdown - Unified LLM Memory

## ğŸ“ Project Structure Overview

```
unified-llm/
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ server.py                  # FastAPI web server entry point
â”œâ”€â”€ unified_llm/              # Main package
â”‚   â”œâ”€â”€ models.py             # Data models (Message, Conversation, Fact)
â”‚   â”œâ”€â”€ factory.py            # Service factory (creates all services)
â”‚   â”œâ”€â”€ rag_engine.py          # RAG orchestration
â”‚   â”œâ”€â”€ importers/            # Chat history importers
â”‚   â”œâ”€â”€ memory/               # Fact extraction
â”‚   â”œâ”€â”€ storage/              # Vector database & embeddings
â”‚   â”œâ”€â”€ retrieval/           # Semantic search
â”‚   â””â”€â”€ graph/                # Knowledge graph & persona
â””â”€â”€ web-ui/                   # Next.js frontend
```

---

## ğŸ”· Module Breakdown

### 1. **models.py** - Data Structures
**Purpose:** Defines the core data models used throughout the application.

**What it contains:**
- `Message`: Represents a single chat message (user/assistant/system)
- `Conversation`: A collection of messages with metadata
- `Fact`: Extracted knowledge fact with category and metadata

**Connections:**
- Used by: ALL modules (importers, extractor, storage, graph)
- No dependencies on other modules

**Example:**
```python
Message(role="user", content="I prefer Python over Java")
Conversation(id="123", title="Python Discussion", messages=[...])
Fact(content="Prefers Python", category="preference")
```

---

### 2. **factory.py** - Service Initialization
**Purpose:** Creates and initializes all services using the Factory pattern (Singleton).

**What it does:**
1. Creates embedding service (for converting text to vectors)
2. Creates vector store (Pinecone or mock)
3. Creates LLM client (DeepSeek API)
4. Creates extractor (for fact extraction)
5. Creates retriever (for semantic search)
6. Creates graph service (for knowledge graph)
7. Creates persona engine (for persona generation)

**Key Class:**
- `ServiceFactory`: Singleton that manages all service creation

**Connections:**
- **Depends on:** All other modules (imports them)
- **Used by:** `main.py`, `server.py`
- **Returns:** Dictionary of all initialized services

**Initialization Order:**
```
1. EmbeddingService â†’ 2. VectorStore â†’ 3. LLMClient 
â†’ 4. MemoryExtractor â†’ 5. RetrievalService 
â†’ 6. GraphService â†’ 7. PersonaEngine
```

---

### 3. **importers/** - Chat History Import
**Purpose:** Parse and convert chat export files into standardized `Conversation` objects.

#### **base_importer.py**
- Abstract base class defining the importer interface
- All importers must implement `import_data(file_path) -> List[Conversation]`

#### **chatgpt_importer.py**
- Parses ChatGPT export JSON format
- Handles ChatGPT's tree structure (mapping nodes)
- Extracts messages from conversation threads
- **Returns:** List of `Conversation` objects

#### **claude_importer.py**
- Parses Claude export JSON format
- Handles Claude's linear message structure
- **Returns:** List of `Conversation` objects

**Connections:**
- **Depends on:** `models.py` (uses Message, Conversation)
- **Used by:** `main.py`, `server.py` (import endpoint)

**Flow:**
```
JSON File â†’ Importer â†’ List[Conversation] â†’ Memory Extractor
```

---

### 4. **memory/extractor.py** - Fact Extraction
**Purpose:** Uses LLM to extract structured facts from chat messages.

**Key Classes:**
- `LLMClient`: Wrapper around OpenAI/DeepSeek API
  - `generate_async()`: Sends messages to LLM, gets response
  - `generate()`: Synchronous wrapper
  
- `MemoryExtractor`: Extracts facts from messages
  - `extract_facts_async()`: Main extraction method
  - Filters out short/greeting messages
  - Batches messages for efficiency
  - Uses LLM to identify facts (preferences, projects, etc.)

**How it works:**
1. Filters messages (removes "hi", "thanks", very short messages)
2. Batches messages together (10 per batch)
3. Sends batch to LLM with prompt: "Extract facts from these messages"
4. LLM returns facts in format: `[INDEX] Fact: ... Category: ...`
5. Parses response and creates `Fact` objects

**Connections:**
- **Depends on:** `models.py` (Message, Fact), LLM API
- **Used by:** `main.py`, `server.py` (during import)
- **Output:** List of `Fact` objects

**Flow:**
```
List[Message] â†’ Filter â†’ Batch â†’ LLM â†’ Parse â†’ List[Fact]
```

---

### 5. **storage/** - Vector Database & Embeddings
**Purpose:** Store facts as vectors for semantic search.

#### **embeddings.py**
- Converts text to numerical vectors (embeddings)
- `LocalEmbeddingService`: Uses sentence-transformers (all-MiniLM-L6-v2)
  - Runs locally, no API needed
  - Returns 384-dimensional vectors
- `OpenAIEmbeddingService`: Uses OpenAI API (not currently used)
- `MockEmbeddingService`: Returns dummy vectors for testing

**Connections:**
- **Used by:** `vector_store.py`
- **No dependencies** on other project modules

#### **vector_store.py**
- Stores facts in vector database (Pinecone or mock)
- **Key Methods:**
  - `add_facts()`: Converts facts to embeddings, stores in Pinecone
  - `search()`: Finds similar facts using semantic search

**How it works:**
1. Takes list of `Fact` objects
2. Converts to text: `"{category}: {content}"`
3. Generates embeddings using `EmbeddingService`
4. Stores in Pinecone with metadata (category, timestamp, etc.)
5. If no Pinecone API key, uses mock storage (in-memory list)

**Connections:**
- **Depends on:** `embeddings.py`, `models.py` (Fact)
- **Used by:** `retrieval/retriever.py`, `graph/service.py`
- **Stores:** Facts as vectors for later retrieval

**Flow:**
```
List[Fact] â†’ Convert to Text â†’ Generate Embeddings â†’ Store in Pinecone
```

---

### 6. **retrieval/retriever.py** - Semantic Search
**Purpose:** Retrieves relevant facts for a user query.

**Key Class:**
- `RetrievalService`: Finds similar facts to a query

**How it works:**
1. Takes user query string
2. Uses `VectorStore.search()` to find similar facts
3. Returns list of relevant facts (sorted by similarity)

**Connections:**
- **Depends on:** `storage/vector_store.py`
- **Used by:** `rag_engine.py`
- **Returns:** List of `Fact` objects (or strings, depending on method)

**Flow:**
```
User Query â†’ Generate Query Embedding â†’ Search Vector Store â†’ Return Similar Facts
```

---

### 7. **rag_engine.py** - RAG Orchestration
**Purpose:** Combines retrieval and generation to answer queries.

**Key Class:**
- `RAGEngine`: Orchestrates the RAG pipeline

**How it works:**
1. Takes user query
2. Retrieves relevant facts using `RetrievalService`
3. Builds context string from retrieved facts
4. Creates prompt with context + user query
5. Sends to LLM to generate answer
6. Returns answer + retrieved facts

**Connections:**
- **Depends on:** `retrieval/retriever.py`, `memory/extractor.py` (LLMClient)
- **Used by:** `main.py`, `server.py` (query endpoint)
- **Returns:** `RAGResponse` (answer + facts)

**Flow:**
```
User Query â†’ Retrieve Facts â†’ Build Context â†’ LLM Generation â†’ Answer
```

---

### 8. **graph/** - Knowledge Graph & Persona
**Purpose:** Visualizes facts as a graph and generates persona summaries.

#### **service.py**
- `GraphService`: Builds knowledge graph from facts
- Uses NetworkX to create graph structure
- **Key Methods:**
  - `build_graph()`: Fetches all facts, creates nodes
  - `get_graph_data()`: Returns nodes/links for visualization

**How it works:**
1. Fetches all facts from vector store
2. Creates graph nodes (one per fact)
3. Adds edges (currently minimal - MVP)
4. Returns data for frontend visualization

**Connections:**
- **Depends on:** `storage/vector_store.py`, `models.py`
- **Used by:** `server.py` (graph endpoint), `persona.py`

#### **persona.py**
- `PersonaEngine`: Generates user persona from facts
- **How it works:**
  1. Gets all facts from graph
  2. Groups by category
  3. Sends to LLM: "Generate persona from these facts"
  4. Returns structured persona (bio, traits, skills, etc.)

**Connections:**
- **Depends on:** `graph/service.py`, `memory/extractor.py` (LLMClient)
- **Used by:** `server.py` (persona endpoint)

---

### 9. **main.py** - CLI Entry Point
**Purpose:** Command-line interface for importing and querying.

**What it does:**
1. Parses command-line arguments
2. Initializes services via Factory
3. If `--import-file`: Imports chat history, extracts facts, stores them
4. If `--query`: Queries the system using RAG
5. If `--interactive`: Interactive chat mode

**Connections:**
- **Depends on:** `factory.py`, `rag_engine.py`, `importers/`
- **Uses:** All services for import and query

**Usage:**
```bash
python main.py --import-file chat.json --type chatgpt
python main.py --query "What are my preferences?"
python main.py --interactive
```

---

### 10. **server.py** - Web API Server
**Purpose:** FastAPI server that provides REST API for frontend.

**Endpoints:**
- `GET /health` - Health check
- `GET /stats` - System statistics (total facts, storage type)
- `POST /query` - Query memory (uses RAG)
- `GET /facts` - List stored facts
- `POST /import` - Import chat history (background task)
- `GET /graph` - Get knowledge graph data
- `GET /persona` - Generate persona summary

**Connections:**
- **Depends on:** `factory.py`, `rag_engine.py`, `importers/`
- **Used by:** Frontend (web-ui) via HTTP requests

**Flow:**
```
Frontend Request â†’ FastAPI Endpoint â†’ Service â†’ Response
```

---

### 11. **web-ui/** - Frontend Application
**Purpose:** Next.js React frontend for user interface.

**Pages:**
- `/` - Landing page with stats
- `/import` - Import chat history
- `/chat` - Query interface
- `/memory` - Browse stored facts

**Connections:**
- **Depends on:** `server.py` (API calls via axios)
- **No direct Python dependencies**

---

## ğŸ”„ Program Flow

### Flow 1: Importing Chat History

```
1. User uploads JSON file (ChatGPT/Claude export)
   â†“
2. server.py receives file â†’ Background task
   â†“
3. Importer (ChatGPTImporter/ClaudeImporter) parses JSON
   â†“
4. Returns List[Conversation] objects
   â†“
5. For each Conversation:
   â†“
6. MemoryExtractor.extract_facts_async()
   - Filters messages (removes short/greeting)
   - Batches messages (10 per batch)
   - Sends to LLM: "Extract facts from these messages"
   - LLM returns facts in structured format
   - Parses and creates Fact objects
   â†“
7. VectorStore.add_facts()
   - Converts facts to text: "{category}: {content}"
   - Generates embeddings (384-dim vectors)
   - Stores in Pinecone with metadata
   â†“
8. Facts are now searchable in vector database
```

### Flow 2: Querying the System (RAG)

```
1. User asks question: "What are my preferences?"
   â†“
2. RAGEngine.generate_response()
   â†“
3. RetrievalService.retrieve()
   - Converts query to embedding
   - Searches VectorStore for similar facts
   - Returns top 5 most relevant facts
   â†“
4. RAGEngine builds context:
   "Retrieved Facts:
   - preference: I prefer Python over Java
   - preference: I like working with React
   ..."
   â†“
5. RAGEngine sends to LLM:
   System: "You are a helpful assistant with access to user's memory.
            Use these facts to answer: [facts]"
   User: "What are my preferences?"
   â†“
6. LLM generates answer using context
   â†“
7. Returns RAGResponse(answer, retrieved_facts)
```

### Flow 3: Building Knowledge Graph

```
1. User requests graph visualization
   â†“
2. GraphService.build_graph()
   â†“
3. Fetches all facts from VectorStore
   â†“
4. Creates NetworkX graph:
   - Each fact = node
   - Edges = relationships (MVP: minimal)
   â†“
5. Returns graph data (nodes + links)
   â†“
6. Frontend visualizes using react-force-graph
```

### Flow 4: Generating Persona

```
1. User requests persona
   â†“
2. PersonaEngine.generate_persona()
   â†“
3. Gets all facts from GraphService
   â†“
4. Groups facts by category
   â†“
5. Sends to LLM:
   "Generate persona from these facts:
   ## preference
   - I prefer Python
   - I like React
   ..."
   â†“
6. LLM returns structured JSON:
   {
     "bio": "...",
     "traits": {...},
     "key_themes": [...]
   }
   â†“
7. Returns persona to frontend
```

---

## ğŸ”— Module Dependency Graph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  models.py  â”‚ (No dependencies - base data structures)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ importers/  â”‚                              â”‚  factory.py â”‚
â”‚             â”‚                              â”‚             â”‚
â”‚ - base      â”‚                              â”‚ Creates all â”‚
â”‚ - chatgpt   â”‚                              â”‚ services    â”‚
â”‚ - claude    â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
       â”‚                                            â”‚
       â”‚                                            â”œâ”€â”€â–º storage/
       â”‚                                            â”‚    - embeddings.py
       â”‚                                            â”‚    - vector_store.py
       â”‚                                            â”‚
       â”‚                                            â”œâ”€â”€â–º memory/
       â”‚                                            â”‚    - extractor.py
       â”‚                                            â”‚
       â”‚                                            â”œâ”€â”€â–º retrieval/
       â”‚                                            â”‚    - retriever.py
       â”‚                                            â”‚
       â”‚                                            â”œâ”€â”€â–º graph/
       â”‚                                            â”‚    - service.py
       â”‚                                            â”‚    - persona.py
       â”‚                                            â”‚
       â”‚                                            â””â”€â”€â–º rag_engine.py
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  main.py    â”‚                              â”‚ server.py   â”‚
â”‚ (CLI)       â”‚                              â”‚ (API)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   web-ui/     â”‚
                                              â”‚  (Frontend)    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Design Patterns

### 1. **Factory Pattern** (`factory.py`)
- Centralized service creation
- Singleton ensures one instance
- Lazy initialization

### 2. **Strategy Pattern** (Importers)
- Base class defines interface
- Different implementations (ChatGPT, Claude)
- Easy to add new importers

### 3. **Dependency Injection**
- Services passed to constructors
- Easy to test (can inject mocks)
- Loose coupling

### 4. **RAG Pattern** (`rag_engine.py`)
- Retrieve â†’ Augment â†’ Generate
- Standard pattern for LLM applications

---

## ğŸ“Š Data Flow Summary

```
Import Flow:
JSON â†’ Conversation â†’ Message â†’ Fact â†’ Embedding â†’ Vector DB

Query Flow:
Query â†’ Embedding â†’ Vector Search â†’ Facts â†’ Context â†’ LLM â†’ Answer

Graph Flow:
Vector DB â†’ Facts â†’ Graph Nodes â†’ Visualization

Persona Flow:
Graph â†’ Facts â†’ LLM â†’ Persona JSON
```

---

## ğŸ”‘ Key Concepts Explained

### **Embeddings**
- Text converted to numbers (vectors)
- Similar text = similar vectors
- Enables semantic search (find meaning, not exact words)

### **Vector Database (Pinecone)**
- Stores vectors (embeddings)
- Fast similarity search
- Returns most similar items to a query

### **RAG (Retrieval-Augmented Generation)**
- Retrieve relevant context first
- Then generate answer with context
- More accurate than LLM alone

### **Knowledge Graph**
- Facts as nodes
- Relationships as edges
- Visual representation of knowledge

---

This architecture allows the system to:
1. Import from multiple platforms
2. Extract structured knowledge
3. Store for semantic search
4. Retrieve relevant context
5. Generate personalized responses
6. Visualize knowledge
7. Generate persona summaries

